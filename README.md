# Automated ELT Pipeline with Airflow, dbt, and PostgreSQL

This project demonstrates how to build a modern data pipeline using **Apache Airflow**, **dbt**, **PostgreSQL**, and **Docker**. The pipeline extracts data from an external API, loads it into a Postgres database, transforms it using dbt, and orchestrates everything through Airflow.

> ğŸ¥ **Based on tutorial by Data Engineer One**  
> https://www.youtube.com/watch?v=vMgFadPxOLk

---

## ğŸš€ Tech Stack

| Tool             | Description                                                   |
|------------------|---------------------------------------------------------------|
| Apache Airflow   | Manages and schedules the ETL workflow                        |
| PostgreSQL       | Stores raw and processed data                                 |
| dbt              | SQL-based transformation layer inside the data warehouse      |
| Docker           | Containerizes the application for easy deployment             |
| Python + Pandas  | Extracts and stages data from APIs                            |

---

## ğŸ“ Project Structure

    .
    â”œâ”€â”€ dags/                  # Airflow DAGs
    â”œâ”€â”€ dbt/                   # dbt project (models, profiles)
    â”œâ”€â”€ scripts/               # Python scripts for extraction
    â”œâ”€â”€ Dockerfile             # Docker image with dependencies
    â”œâ”€â”€ docker-compose.yml     # Service orchestration
    â”œâ”€â”€ requirements.txt       # Python dependencies
    â””â”€â”€ README.md              # Project documentation

---

## ğŸ› ï¸ Setup Instructions

1. Clone the repository

       git clone https://github.com/your-username/your-repo.git
       cd your-repo

2. Build and start services

       docker-compose up --build

3. Access Airflow UI

    - URL: http://localhost:8080
    - Username: airflow
    - Password: airflow

4. Configure Airflow Connections in Admin â†’ Connections:

    - ID: postgres_default
    - Conn Type: Postgres
    - Host: postgres
    - Schema: airflow
    - Login: airflow
    - Password: airflow
    - Port: 5432

5. Trigger the DAG (`elt_pipeline`) from the Airflow UI

---

## âœ… dbt Usage

Run the following inside the dbt container:

    dbt run
    dbt test
    dbt docs generate
    dbt docs serve

Directory structure:

    dbt/
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ staging/
    â”‚   â””â”€â”€ marts/
    â”œâ”€â”€ dbt_project.yml
    â””â”€â”€ profiles.yml

---

## ğŸ” Monitoring & Testing

- Airflow handles scheduling and retries.
- dbt provides:
  - Data tests (`not_null`, `unique`, `relationships`)
  - Lineage and documentation via dbt docs

---

## ğŸ“š References

- [YouTube Tutorial](https://www.youtube.com/watch?v=vMgFadPxOLk)
- [dbt Documentation](https://docs.getdbt.com/)
- [Airflow Documentation](https://airflow.apache.org/docs/)
- [Docker Documentation](https://docs.docker.com/)

---

## ğŸ™Œ Author & Credits

- Based on the tutorial by **Data Engineer One**
- README improvements by the community

---

## ğŸ“„ License

This project is licensed under the MIT License.
