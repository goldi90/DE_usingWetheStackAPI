# Automated ELT Pipeline with Airflow, dbt, and PostgreSQL

This project demonstrates how to build a modern data pipeline using **Apache Airflow**, **dbt**, **PostgreSQL**, and **Docker**. The pipeline extracts data from an external API, loads it into a Postgres database, transforms it using dbt, and orchestrates everything through Airflow.

> ğŸ¥ **Based on tutorial by Data Engineer One**  
> [How to build an automated data pipeline using Airflow, dbt, Postgres](https://www.youtube.com/watch?v=vMgFadPxOLk)

---

## ğŸš€ Tech Stack

| Tool           | Description                                           |
|----------------|-------------------------------------------------------|
| **Apache Airflow** | Manages and schedules the ETL workflow              |
| **PostgreSQL**     | Relational database for storing raw and processed data |
| **dbt**            | Transforms data within Postgres using SQL and manages data models |
| **Docker**         | Containers for Airflow, dbt, Postgres, etc.        |
| **Python + Pandas**| Extracts data from API and stages to Postgres      |

---

## ğŸ“ Project Structure

â”œâ”€â”€ dags/ # Airflow DAGs for orchestrating the pipeline
â”œâ”€â”€ dbt/ # dbt project directory (models, tests, etc.)
â”œâ”€â”€ scripts/ # Python scripts for data extraction
â”œâ”€â”€ Dockerfile # Docker image with necessary dependencies
â”œâ”€â”€ docker-compose.yml # Orchestrates containers for Airflow, dbt, Postgres
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project documentation
